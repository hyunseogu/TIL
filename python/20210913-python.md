# Pandas
> - 데이터 과학자를 위해 **테이블형태**로 데이터를 다룰 수 있게 해주는 패키지(python용 엑셀)
> - 기존 데이터처리 라이브러리인 numpy 대신 주로 사용
> - 일반인이 데이터분석을 접하기 쉽게 만들어준 결정적인 라이브러리
> - pandas만으로도 충분히 데이터 분석이 가능할 정도로 고수준의 함수들을 내장
> - 앞으로 진행하는 데이터분석 과정에서 주로 사용하게 될 데이터구조

## DataFrame
> - 엑셀에 익숙한 사용자를 위해 제작 된 **테이블형태의 데이터 구조**  
> - 다양한 형태의 데이터를 받아 사용할 수 있으며 다양한 **통계, 시각화 함수를 제공**한다.  

실제 데이터를 불러들이고 값을 확인 해 보며 기본적인 pandas 사용법을 익혀보도록 하겠습니다.

### 데이터 불러오기
pandas는 다양한 데이터 파일 형태를 지원하며 주로 csv, xlsx, sql, json을 사용합니다.
    
> **`read_csv()`**  
**`read_excel()`**  
**`read_sql()`**  
**`read_json()`**  
**`json_normalize()`**


```python
df = pd.read_csv('./data/loan1.csv')
df1 = pd.read_excel('./data/loan1.xlsx')


# 만약 모듈을 찾을 수 없는 오류가 발생한다면 추가 모듈 설치
# !pip install xlrd
# 엑셀파일에 시트에 따라 데이터 구분이 지어진 경우 시트별로 데이터 프레임 제작 가능
# 다른 엑셀파일 형식을 가져올 때 engine 파라메터 추가해주시면 됩니다.
df1 = pd.read_excel('./data/loan1.xlsb',
                   sheet_name='구매영수증상세+상품마스터포함',
                   engine='pyxlsb',
                   encoding='utf-8'
                   )
'''
'c'
'python' - 컬럼에 한글이 포함되거나 파일명에 한글이 포함되어서 데이터 로딩이 잘 안될 때
'xlrd'
'xlsb'
'''

# 데이터베이스로 부터 자료 읽기
!pip install pymysql

import pymysql

# 서버에 접속하는 정보를 저장해줌
con = pymysql.connect(host='db서버주소', port=3306, user='id', passwd='pwd', db='dbname')

# 쿼리 작성
query = 'select * from table'
db_df = pd.read_sql(query, con = con)
```

### 데이터 저장하기
불러들인 혹은 작업을 마친 데이터프레임을 다양한 파일형태로 저장이 가능합니다.
    
> **`to_csv()`**  
**`to_excel()`**  
**`to_sql()`**

```python
# index = False 파라메터는 기존 데이터프레임의 인덱스를 무시하고 저장
# csv.gz 로 저장하면 데이터 파일을 압축
df.to_csv('./data/save_test.csv.gz', index = False)


test_df = pd.read_csv('./data/save_test.csv.gz')
test_df
```

### 사용 데이터 간략 설명
> 미국 핀테크 회사인 lending club의 대출 데이터베이스  
클라우드펀딩과 대출을 결합한 핀테크의 시초라고 부를 수 있는 회사  
방대한 양의 대출정보를 공개하면서 금융정보분석에도 기여한 공이 큰 데이터  
2007 ~ 2015 년 대출정보 및 개인정보를 담고 있음  
226만건, 145항목 정보를 담고있음  
실습데이터는 이 중 4만건을 추출한 데이터를 사용합니다.  

데이터출처: https://www.kaggle.com/wordsforthewise/lending-club

### 데이터 살펴보기

```python
# 데이터를 불러들인 후 가장 처음 하는 작업
# 데이터의 구조, 형태 파악하기
# 데이터의 첫 5개 샘플 확인하기
df.head()

# 10개를 확인하려면?
df.head(10)

# 데이터의 마지막 5개 샘플 확인하기
# 데이터가 잘 가져왔는지 확인 할 때 보통 씁니다.
df.tail()

# 데이터의 갯수를 살펴봅니다
len(df)

# 데이터의 전반적인 정보를 확인합니다.
df.info()

# 데이터의 기초통계량을 확인합니다.
df.describe()

# numpy 함수로 데이터 shape 확인
np.shape(df)

# 인덱스
df.index

# 컬럼
df.columns
```
데이터셋을 살펴 본 결과 정체를 알 수 없는 많은 컬럼이 있는 걸 확인했고, 

20000개의 샘플이 불러들여진 것을 확인 할 수 있었습니다.

추가로 데이터 중간 중간 비어있는 값도 있는 것을 확인했습니다.

### 데이터접근 (인덱싱, 슬라이싱, 샘플링)
```python

# 첫 샘플 혹은 레코드(대출건)에 대한 데이터를 살펴보겠습니다.
# 인덱스넘버로 데이터에 접근하는 .iloc[색인]
# 각 컬럼이나, 행단위 접근했을 때 출력되는 벡터 데이터를 Series (시리즈) 라고 하는 자료구조
df.iloc[0]

# index, values로 각각의 속성에 접근 가능
df.iloc[0].values

# 10번 인덱스 부터 20번 인덱스 샘플 접근
# start, end+1, steps
df.iloc[10:20:2]

# 첫번째 0, 10, 20 인덱스 샘플 접근
df.iloc[[0, 10, 20]]
df.take([0, 10, 20])

# 컬럼 단위 샘플 접근
# 열벡터 -> 시리즈
# df[텍스트형태의 컬럼명]
df['emp_title']

# 여러 컬럼 동시 접근
df[['emp_title', 'int_rate']]

# row와 columns을 동시에 슬라이싱 하는 속성
# df.loc[인덱스, 컬럼명]
df.loc[100:200, ['emp_title', 'int_rate']]


df.loc[100:200, df.columns[0:10]]

# df의 컬럼명을 순환하면서 컬럼단위로 접근하고 각 컬럼의 고유값을 출력해주는 코드
for col_nm in df.columns:
    print(col_nm, df[col_nm].unique())

# 고유값 개수 출력
for col_nm in df.columns:
    print(col_nm, df[col_nm].nunique())

```

### 팬시인덱싱
> **`bool`** 형태의 array를 조건을 전달하여 다차원 배열을 인덱싱하는 방법.  
조건식을 사용하여 분석에 필요한 데이터샘플을 추출하기 용이합니다.


```python
# 신용등급이 A인 샘플의 emp_title 확인
df[df['grade'] == 'A']['emp_title']

# 대출금액평균
df['loan_amnt'].mean()

# 신용등급 A와 B인 샘플접근
# 조건식을 여러개 써야 한다면 조건마다 ()로 감싸주시는 것이 좋습니다.
df[(df['grade'] == 'A') | (df['grade'] == 'B')]

# df loan_amnt 컬럼값이 10000이상인 채권샘플의 grade
df[df['loan_amnt'] >= 10000]['grade'].value_counts()

# 인덱스 값으로 정렬
df[df['loan_amnt'] >= 10000]['grade'].value_counts().sort_index()

# df grade C 와 D 인 채권샘플 annual_inc 최대값인 인덱스 빼오기 (idxmax)
df[(df['grade'] == 'C') | (df['grade'] == 'D')]['annual_inc'].idxmax()

# 위의 값을 가진 사람의 직업
df.iloc[df[(df['grade'] == 'C') | (df['grade'] == 'D')]['annual_inc'].idxmax()]['emp_title']
```

## 데이터프레임 병합
> 실제 분석업무를 진행하다보면 데이터가 여기저기 분산되어 있을 경우가 더 많습니다.  
조각난 데이터를 분석에 필요한 데이터셋으로 만들기 위해 데이터프레임 병합을 많이 사용합니다.  
한개 이상의 데이터프레임을 병합 할 때 주로 사용하는 함수 2가지를 알아보겠습니다.

### 데이터 병합에 사용가능한 key(병합할 기준이 되는 행 or 열)값이 있는경우
**`pd.merge`**(베이스데이터프레임, 병합할데이터프레임)  
> 사용 가능 한 파라메터
- `how` : 'left', 'right', 'inner', 'outer'
- `left_on` : key값이 다를 경우 베이스데이터프레임의 key 설정
- `right_on` : key값이 다를 경우 병합데이터프레임의 key 설정
    
### 단순 데이터 연결
**`pd.concat`**([베이스데이터프레임, 병합할데이터프레임], axis=0 or 1)
> 사용 가능 한 파라메터  
- `axis` : 축 방향 설정
### merge 예시
```python

merge_df1 = pd.DataFrame({
    '이름': ['원영', '사쿠라', '유리', '예나', '유진', '나코', '은비', '혜원', '히토미', '채원', '민주', '째욘'],
    '국어': [100, 70, 70, 70, 60, 90, 90, 70, 70, 80, 100, 100],
    '영어': [100, 90, 80, 50, 70, 100, 70, 90, 100, 100, 80, 100]
    }, columns=['이름', '국어', '영어']) 

merge_df2 = pd.DataFrame({
    '일어': [80, 100, 100, 90, 70, 50, 100],
    '수학': [90, 70, 100, 80, 70, 80, 90],
    '이름': ['원영', '사쿠라', '나코', '히토미', '예나', '은비', '째욘'],
    }, columns=['일어', '수학', '이름'])


pd.merge(merge_df1, merge_df2)
pd.merge(merge_df1, merge_df2, how='outer')
```

### concat 예시
현재 df에 저장되어있는 데이터에 추가로 2만개의 데이터를 이어붙여보겠습니다. df1이라는 변수에 이어붙일 데이터를 불러들여 병합을 진행해보겠습니다.

```python

# df1 변수에 loan2.csv 파일을 읽어들입니다.
df1 = pd.read_csv('./data/loan2.csv')

# 데이터프레임 확인
df1.head()

# df 와 df1 shape 확인
df.shape, df1.shape

# 데이터프레임 행단위 병합
pd.concat([df, df1])
```

## 인덱스 편집
방금 전 concat으로 병합한 데이터프레임의 이상한 점을 찾으셨나요?  
데이터 자체는 잘 붙였지만 인덱스가 꼬여있습니다. 인덱스 편집은 데이터분석을 위해 필요한 인덱스를 설정하기 위해 필요합니다.

```python
# 인덱스리셋
# drop - 현재 인덱스의 원본값을 버림
# inplace - 원본값 변경

concat_df = concat_df.reset_index(drop=True)
concat_df.reset_index(drop=True, inplace=True)

# 기존 컬럼값을 취해 index로 사용
concat_df.set_index('id')
```

## 컬럼편집
인덱스편집과 마찬가지로 데이터프레임의 컬럼을 변경해야 할 경우도 있습니다. 데이터프레임은 컬럼단위 샘플링 및 인덱싱, 이름변경이 가능합니다.

### 컬럼선택

```python
df.columns[0]


for i in df.columns:
    print(i)

# columns 속성도 인덱싱 및 슬라이싱이 가능합니다.
df.columns[5:10]

# df의 개인정보에 관한 컬럼만을 색인으로 df를 슬라이싱하고 person_df 변수에 할당
person_df = df[df.columns[:25]]

```
### 컬럼삭제
현재 데이터셋에는 개인식별정보가 지워져서 데이터가 존재하지 않습니다. 불필요한 데이터 column을 지우도록 하겠습니다.

```python

# 지울 column의 데이터값이 모두 NaN인지 확인

person_df.info()

person_df['id'].sum()

person_df['id'].isna().all()

# 컬럼삭제 drop('컬럼명', axis=1)
# del (df['컬럼명'])
# 실제로는 컬럼 및 행도 삭제 가능합니다. axis=0(기본값)
# inplace=True 파라메터를 사용해서 원본값을 변경가능합니다.
# person_df = person_df.drop('id', axis=1)

person_df.drop('id', axis = 1, inplace = True)

# del 메모리 삭제 키워드 사용

del person_df['member_id']

person_df.pop('url')
```



### 컬럼명 변경
    경우에 따라서는 데이터셋 제작 중 컬럼명을 변경해야 할 경우도 있습니다.
    국내 수집 데이터 사용 시 컬럼이 한글일 경우 영어로 변경을 많이 합니다.

```python
# home_ownership을 간략하게 home으로 변경
# 한글도 가능합니다만 권장하지는 않습니다.

person_df.rename(columns={'home_ownership' : 'home'}, inplace=True)
```

## 데이터 샘플링 및 분석
> 데이터병합, 인덱스편집, 컬럼선택만으로도 불필요한 정보를 삭제하고 새롭게 데이터셋을 만들 수 있는것을 확인했습니다.  
위에 학습한 내용도 데이터 샘플링에 속한 내용이지만 지금부터는 데이터셋의 데이터를 살펴보면서 의미있는 데이터를 추려보도록 하겠습니다.  
    
**데이터프레임의 기본적인 인덱싱, 슬라이싱, 조건부 샘플링을 조합하면 데이터의 샘플을 확인 하는 과정만으로도 데이터분석이 가능해집니다.**

```python

# 분석에 필요한 데이터프레임을 만들었으니 원본값을 사용하겠습니다. 기존 df에 person_df 값을 덮어 씌웁니다.
df = person_df.copy()

# 분석에 필요한 데이터셋을 생성했다면 파일로도 저장 해둡시다.
df.to_csv('./data/person_df.csv', index = False)
```

### 저는 채권자의 개인정보에 관심이 많습니다. 고객의 직업을 살펴보겠습니다.

```python
# emp_title 접근
df['emp_title']

# 값을 카운트 하는 함수 value_counts()
df['emp_title'].value_counts()
```

### 데이터프레임 형변환

```python
# Owner, owner 같은 직업이지만 대소문자 구분에 따라 다른 값으로 취급되는 문제가 있네요.
# 대소문자 구분을 없애기 위해 모두 소문자로 데이터값을 변경하겠습니다.
# 소문자 변환 전 혹시모를 int, float 데이터가 있을지 모를 상황에 대비해서 모두 문자열로 변경해주겠습니다.
# 형변환 함수 astype(데이터타입)

df['emp_title'] = df['emp_title'].astype(str)

df['emp_title'] = df['emp_title'].str.upper()

```

### 배운사람들의 코드, 고오급 python 스킬
numpy를 학습하면서 브로드캐스팅에 관하여 잠깐 언급했었습니다. 그렇다면 그 파워풀하다던 브로드캐스팅은 어떻게 사용해야할까요?
    
>기타 언어에서는 지원하지 않는 기능이니만큼 파이썬의 특징을 가장 잘 살리는 코드  
**`apply`** 함수를 사용하여 인자로 받는 모든 데이터에 함수를 적용

#### apply 함수로 컬럼에 적용시키는 코드 구조
    df['컬럼명'] = df['컬럼명'].apply(lambda x: func(x) if 조건문)
    df['컬럼명'] = df['컬럼명'].apply(func_nm)

```python

# 대문자 만드는 함수
def make_upper(x):
    return x.upper()

# apply() 함수사용 반복이 가능한 데이터구조의 모든 인자에 적용
# lambda 각 인자에 적용할 함수 혹은 연산
df['emp_title'] = df['emp_title'].apply(make_upper)

df['emp_title'] = df['emp_title'].apply(lambda x : x.lower())

# 대소문자 구분을 처리한 값 확인
# 기존 value_count 값과 차이가 있음을 확인 할 수 있습니다.
# 제공 된 데이터셋이라도 이와 같은 작은 차이가 있을 수 있습니다.
# 데이터를 꼼꼼하게 살펴볼 수록 디테일한 차이를 만들 수 있습니다.
df['emp_title'].value_counts()[:40]

# owner인 사람들 샘플링
df[df['emp_title'] == 'owner'] 

# 샘플링 된 데이터프레임의 단일 컬럼 접근
df[df['emp_title'] == 'owner']['annual_inc']

# 컬럼 평균값 계산
df[df['emp_title'] == 'owner']['annual_inc'].mean()

# 코드 하나 변경으로 간단한 분석 가능
# owner가 아닌 사람들의 평균
# 컬럼 평균값 계산
df[df['emp_title'] != 'owner']['annual_inc'].mean()

# 각 직업별 평균연봉이 궁금하다 groupby
# 엑셀의 pivol table 과 비슷한 기능
df.groupby('emp_title').mean()

# 위 테이블에서 연간수입 접근
# ascending = False  <- 내림차순
df.groupby('emp_title')['annual_inc'].mean().sort_values(ascending = False)

# pivot table

pd.pivot_table(df,
              index = 'emp_title',
              columns= 'grade',
              values= 'annual_inc',
              aggfunc= np.mean)

```


## 결측치 처리
> 데이터 분석을 위해서는 데이터셋 내에 빈 값이 있는 경우 분석에 방해가 될 수 있는 여지가 많습니다.  
모든 결측치를 없애야 하는 것은 아니지만 되도록이면 결측치를 채우는 방법, 혹은 없애는 방법등으로 결측치를 처리합니다.  
몇가지 예시를 살펴보면서 결측치 처리에 대해 알아봅시다.

```python
# info() 함수는 결측치에 대한 정보도 보여줍니다.
# 컬럼별 isnull() 함수를 사용해도 무방합니다.
df.info()

#    확인결과 emp_title, emp_length, dti에 결측치가 존재합니다.
#    해당 컬럼의 결측치 샘플들을 살펴보고 결측치를 처리해 보겠습니다.

# 컬럼별 결측치 확인을 위한 isnull()함수 리턴값이 bool 형태로 반환되어 조건부 샘플링이 가능합니다.
df['emp_length'].isnull()

# dti 컬럼에 결측치가 존재하는 샘플 확인
len(df[df['dti'].isnull()])

#    직업과 근속연수에 관한 부분은 데이터를 통한 유추나 계산값을 통해 채워넣을 수 있는 항목은 아닌 것 같습니다.
#    다만 dti의 경우 실수로 채워져 있는 부분이니 수업을 위해 평균값 혹은 근사치를 계산하여 채워보도록 하겠습니다.
```

### 결측치 채우기

```python
# dti 컬럼의 NaN값 index 확인
df[df['dti'].isnull()].index

# fillna() 함수로 NaN 값을 dti 컬럼의 평균으로 채우기
df['dti'].fillna(df['dti'].mean(), inplace = True)


# fillna() 함수의 다양한 채우기 방법 파라메터 확인해보기
df['dti'].fillna(method = 'bfill',inplace = Ture)
df['dti'].fillna(method = 'ffill',inplace = Ture)

```

### 결측치 제거
```python

# emp_length 결측치가 있는 샘플 확인
df[df['emp_length'].isnull()]

# 결측치 제거
df.dropna(inplace = True)
```
