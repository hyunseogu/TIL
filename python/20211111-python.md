# 선형 회귀

## Linear Regression

- Hypothesis = Wx+ b
- 사람이 폼을 결정하고 컴퓨터가 W, b를 찾아준다.
- Cost = min \frac{1}{2m}\sum_{i=1}^{m}\left (H_{\theta}(x^{(i)})-(y^{(i)}) \right )^{2}
- (예측값 - 실제값)^2을 최소화

## Ridge Regression

- \frac{1}{2m}\sum_{i=1}^{m}\left (H_{\theta}(x^{(i)})-(y^{(i)}) \right )^{2} + \frac{\lambda }{2}\sum_{j=1}^{n}\theta _{j}^{2}
- Cost 함수에 L2 norm을 추가해 규제

## Lasso Regression

- \frac{1}{2m}\sum_{i=1}^{m}\left (H_{\theta}(x^{(i)})-(y^{(i)}) \right )^{2} + \frac{\lambda }{2}\sum_{j=1}^{n}\left |\theta_{j}  \right |
- Cost 함수에 L1 norm을 추가해 규제
- feature를 선택할때 사용, 강한 규제

## Elastic Net 

- Elastic Net = Ridge + Lasso


